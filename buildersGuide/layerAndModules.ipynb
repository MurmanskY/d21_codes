{"cells":[{"cell_type":"markdown","metadata":{},"source":["## 1. 自定义块"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[ 0.2660,  0.0450,  0.1234, -0.0009, -0.0774, -0.0315,  0.0144,  0.1165,\n","          0.0174,  0.2231],\n","        [ 0.2988,  0.0474,  0.0389,  0.0541,  0.0194, -0.0828, -0.0047,  0.1424,\n","          0.0642,  0.1637]], grad_fn=<AddmmBackward0>)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","from torch import nn\n","from torch.nn import functional as F\n","\n","net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n","\n","X = torch.rand(2, 20)\n","net(X)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["class MLP(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.hidden = nn.Linear(20, 256) # 隐藏层\n","        self.out = nn.Linear(256, 10) # 输出层\n","        \n","    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出\n","    def forward(self, X):\n","        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。\n","        return self.out(F.relu(self.hidden(X)))"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[ 0.0730,  0.1179,  0.1814,  0.0862,  0.1326, -0.2338, -0.2105,  0.0573,\n","          0.1913, -0.1344],\n","        [ 0.0909,  0.0811,  0.1202,  0.0412,  0.2293, -0.2553, -0.1819,  0.0702,\n","          0.1990, -0.2521]], grad_fn=<AddmmBackward0>)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["'''实例化多层感知机'''\n","net = MLP() # 存在默认初始化方法\n","net(X)"]},{"cell_type":"markdown","metadata":{},"source":["## 2. 顺序块"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["'''MySequential类提供了与默认Sequential类相同的功能'''\n","class MySequential(nn.Module):\n","    def __init__(self, *args):\n","        super().__init__()\n","        for idx, module in enumerate(args):\n","            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员\n","            # 变量_modules中。_modules的类型是OrdereDict\n","            self._modules[str(idx)] = module\n","            \n","    def forward(self, X):\n","        # 由于_modules的类型是有序字典，所以可以有序便利其中的元素\n","        for block in self._modules.values():\n","            X = block(X)\n","        return X"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[-0.1861,  0.1401,  0.0224,  0.0664,  0.0032, -0.3142, -0.0633, -0.1514,\n","         -0.1202,  0.0068],\n","        [-0.1935,  0.0154,  0.0150,  0.1118,  0.0492, -0.4230, -0.1266, -0.1531,\n","         -0.1402,  0.1289]], grad_fn=<AddmmBackward0>)"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["'''使用自定义的有序块实现MLP'''\n","net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n","net(X)"]},{"cell_type":"markdown","metadata":{},"source":["## 3. foward函数中执行代码"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["'''例如需要计算一个优化过程中没有更新的变量与参数和输入的运算结果'''\n","class FixedHiddenMLP(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        # 不计算梯度的随机权重参数。在训练期间保持不变\n","        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n","        self.linear = nn.Linear(20, 20)\n","    def forward(self, X):\n","        X = self.linear(X)\n","        # 使用创建的常量参数以及relu和mm函数\n","        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n","        # 复用全连接层。相当于两个全连接层共享参数\n","        X = self.linear(X)\n","        while X.abs().sum() > 1:\n","            X /= 2\n","        return X.sum()\n","        "]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(-0.1229, grad_fn=<SumBackward0>)"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["net = FixedHiddenMLP()\n","net(X) # 返回的就是forward函数中的结果"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(0.1990, grad_fn=<SumBackward0>)"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["'''可以混搭各种组合块的方法'''\n","class NestMLP(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(20, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 32),\n","            nn.ReLU()\n","        )\n","        self.linear = nn.Linear(32, 16)\n","        \n","    def forward(self, X):\n","        return self.linear(self.net(X))\n","\n","chimera = nn.Sequential(\n","    NestMLP(),\n","    nn.Linear(16, 20),\n","    FixedHiddenMLP()\n",")\n","chimera(X)"]}],"metadata":{"kernelspec":{"display_name":"d2l","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":2}
