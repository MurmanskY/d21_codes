{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数管理\n",
    "\n",
    "- 访问参数，用于调试，诊断和可视化\n",
    "- 参数初始化\n",
    "- 在不同模型组件间共享参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2505],\n",
       "        [0.1299]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''具有隐藏层的多层感知机'''\n",
    "import torch\n",
    "from torch import nn\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(4, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, 1)\n",
    ")\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 参数访问\n",
    "\n",
    "当通过sequential类定义模型时，可以通过索引访问模型任意层。每层的参数都在属性中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'weight': tensor([[ 0.3294,  0.1147,  0.2975,  0.1485,  0.2110, -0.0629,  0.2663, -0.3516]]), 'bias': tensor([0.0438])})\n",
      "Parameter containing:\n",
      "tensor([[ 0.3294,  0.1147,  0.2975,  0.1485,  0.2110, -0.0629,  0.2663, -0.3516]],\n",
      "       requires_grad=True)\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "'''检查第二个全连接层的参数'''\n",
    "print(net[2].state_dict())\n",
    "print(net[2].weight)\n",
    "print(type(net[2].weight.data)) # weights中有两个属性，可以通过data访问存有权重的张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 目标参数\n",
    "\n",
    "参数是复合的对象，包含值、梯度和额外信息。 这就是我们需要显式参数值的原因。 除了值之外，我们还可以访问每个参数的梯度。 在上面这个网络中，由于我们还没有调用反向传播，所以参数的梯度处于初始状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([0.0438], requires_grad=True)\n",
      "tensor([0.0438])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(net[2].bias)) # 符合对象，包含值，梯度\n",
    "print(net[2].bias) # 符合对象，包含值，梯度\n",
    "print(net[2].bias.data) # 访问存有权重值的张量\n",
    "\n",
    "net[2].weight == None # 还没有进行反向传播，所以目前还有梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 一次性访问所有参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "'''访问全连接层的参数，与，访问所有层的参数'''\n",
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()]) # 使用*号将列表元素解包为单独的参数传递给print函数\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0438])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''另一种访问网络参数的方法'''\n",
    "net.state_dict()['2.bias'].data # 访问2层偏置值"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
